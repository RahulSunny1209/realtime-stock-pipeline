# ğŸ“ˆ Real-Time Stock Market Data Pipeline

> A production-grade, end-to-end data engineering project demonstrating real-time data streaming, processing, and visualization.

[![GitHub](https://img.shields.io/badge/github-repo-blue)](https://github.com/RahulSunny1209/realtime-stock-pipeline)

**ğŸŒ Live Demo:** [https://stock-pipeline.streamlit.app/](https://stock-pipeline.streamlit.app/)  
**ğŸ“¦ GitHub:** [View Full Code](https://github.com/RahulSunny1209/realtime-stock-pipeline)

---

## ğŸ¯ Quick Links

- **[Try the Live Dashboard](https://stock-pipeline.streamlit.app/)** â­
- **[Watch Demo Video](#)** ğŸ“º
- **[Read Blog Series](#)** ğŸ“
- **[View Architecture](#architecture)** ğŸ—ï¸

---

## ğŸ¯ Project Highlights

This project demonstrates **production-grade data engineering skills**:

### ğŸš€ Real-Time Data Streaming
- Fetches live stock data from **Finnhub API**
- Streams through **Apache Kafka** (3 partitions)
- <30 second latency end-to-end
- Tracks 5 major stocks: AAPL, GOOGL, MSFT, AMZN, TSLA

### âš¡ Stream Processing
- **Apache Spark** Structured Streaming
- 30-second micro-batches
- Exactly-once processing semantics
- Automatic checkpointing for fault tolerance

### ğŸ’¾ Data Storage
- **PostgreSQL** for time-series data
- **Redis** for hot data caching (5-min TTL)
- Optimized indexes for millisecond queries
- Handles 245+ records in first 5 minutes

### ğŸ“Š Interactive Visualization
- **Streamlit** dashboard with 3 tabs
- Real-time auto-refresh
- **Plotly** interactive charts
- CSV export functionality
- System health monitoring

### ğŸ³ Containerization
- **Docker Compose** orchestration
- 9 microservices
- One-command deployment: `docker-compose up -d`
- Health checks and auto-restart

---

## ğŸ—ï¸ Architecture
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Finnhub    â”‚â”€â”€â”€â–¶â”‚  Kafka  â”‚â”€â”€â”€â–¶â”‚  Spark  â”‚â”€â”€â”€â–¶â”‚ PostgreSQL   â”‚â”€â”€â”€â–¶â”‚ Dashboard â”‚
â”‚     API     â”‚    â”‚ (3 part)â”‚    â”‚Streamingâ”‚    â”‚(Time-series) â”‚    â”‚(Streamlit)â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     30s              instant        30s batch       persistent          live view
                                         â”‚
                                         â–¼
                                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                    â”‚  Redis  â”‚
                                    â”‚ (Cache) â”‚
                                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ› ï¸ Technology Stack

| Layer | Technology | Version | Purpose |
|-------|-----------|---------|---------|
| **API** | Finnhub API | v1 | Live stock market data |
| **Streaming** | Apache Kafka | 7.5.0 | Distributed message queue |
| **Processing** | Apache Spark | 3.5.0 | Real-time stream processing |
| **Database** | PostgreSQL | 16 | Time-series data storage |
| **Cache** | Redis | 7 | Hot data caching |
| **Dashboard** | Streamlit | 1.29.0 | Interactive visualization |
| **Charts** | Plotly | 5.18.0 | Interactive graphs |
| **Container** | Docker | Latest | Containerization |
| **Orchestration** | Docker Compose | Latest | Multi-container management |

**Language:** Python 3.11  
**Platform:** Mac M2 (ARM64) compatible

---

## ğŸš€ Local Setup

### Prerequisites
- Docker Desktop 4.0+
- 8GB+ RAM
- Python 3.11+
- Finnhub API Key ([Get free key](https://finnhub.io/register))

### Quick Start
```bash
# Clone repository
git clone https://github.com/RahulSunny1209/realtime-stock-pipeline.git
cd realtime-stock-pipeline

# Add your API key
echo "FINNHUB_API_KEY=your_key_here" > .env

# Start the pipeline
docker-compose up -d

# Open dashboard
open http://localhost:8501
```

**That's it! Your pipeline is running locally!** ğŸ‰

---

## ğŸ“Š Performance Metrics

| Metric | Value |
|--------|-------|
| **End-to-End Latency** | <30 seconds |
| **Data Freshness** | 30-second updates |
| **Throughput** | 1-5 records/sec per stock |
| **Storage Rate** | ~50 records/stock in 5 mins |
| **Dashboard Refresh** | 3-second intervals |
| **Kafka Partitions** | 3 (parallel processing) |
| **Spark Batch Interval** | 30 seconds |
| **Uptime** | 99.9% (auto-restart) |

---

## ğŸ“ Project Structure
```
realtime-stock-pipeline/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ producer/              # Finnhub â†’ Kafka producer
â”‚   â”‚   â””â”€â”€ stock_producer.py
â”‚   â”œâ”€â”€ processing/            # Spark stream processor
â”‚   â”‚   â””â”€â”€ spark_processor_with_storage.py
â”‚   â”œâ”€â”€ storage/               # Database clients
â”‚   â”‚   â””â”€â”€ database.py
â”‚   â”œâ”€â”€ dashboard/             # Streamlit dashboard
â”‚   â”‚   â”œâ”€â”€ app.py            # Full version (local)
â”‚   â”‚   â””â”€â”€ app_render.py     # Simplified (cloud)
â”‚   â””â”€â”€ utils/                 # Utilities
â”‚       â””â”€â”€ logger.py
â”œâ”€â”€ config/                    # Configuration files
â”‚   â”œâ”€â”€ kafka_config.py
â”‚   â”œâ”€â”€ spark_config.py
â”‚   â””â”€â”€ db_config.py
â”œâ”€â”€ scripts/                   # Database schema
â”‚   â””â”€â”€ init_db.sql
â”œâ”€â”€ tests/                     # Unit & integration tests
â”‚   â”œâ”€â”€ test_producer.py
â”‚   â”œâ”€â”€ test_kafka.py
â”‚   â””â”€â”€ test_postgres.py
â”œâ”€â”€ docs/                      # Documentation
â”‚   â”œâ”€â”€ screenshots/
â”‚   â”œâ”€â”€ blog-posts/
â”‚   â””â”€â”€ ARCHITECTURE.md
â”œâ”€â”€ docker-compose.yml         # Container orchestration
â”œâ”€â”€ Dockerfile.producer        # Producer container
â”œâ”€â”€ Dockerfile.spark           # Spark container
â”œâ”€â”€ Dockerfile.dashboard       # Dashboard container
â”œâ”€â”€ requirements.txt           # Python dependencies
â””â”€â”€ README.md                  # This file
```

---

## ğŸ“ What I Learned

### Technical Skills Gained
- âœ… Real-time data streaming architecture
- âœ… Apache Kafka producer/consumer patterns
- âœ… Spark Structured Streaming with micro-batches
- âœ… Time-series database design and optimization
- âœ… Docker containerization and orchestration
- âœ… System monitoring and debugging
- âœ… Production deployment (Streamlit Cloud)

### Engineering Best Practices
- âœ… Fault-tolerant design (retries, checkpointing)
- âœ… Exactly-once processing semantics
- âœ… Performance optimization (indexing, caching)
- âœ… Production-ready code (logging, error handling)
- âœ… Comprehensive documentation
- âœ… Interactive data visualization
- âœ… Free cloud deployment

---

## ğŸ” Key Features

### 1. Real-Time Data Ingestion
- Fetches live stock prices every 30 seconds
- Retry logic with exponential backoff
- Error handling and logging
- Rate limit management

### 2. Fault-Tolerant Streaming
- Kafka topic with 3 partitions
- Message persistence and replay
- Exactly-once semantics
- Automatic recovery

### 3. Scalable Processing
- Spark micro-batching (30s intervals)
- Windowed aggregations
- Checkpointing for state recovery
- Parallelized processing

### 4. Optimized Storage
- PostgreSQL with composite indexes
- Redis caching layer (5-min TTL)
- Time-series data modeling
- Efficient query patterns

### 5. Interactive Dashboard
- Real-time price updates
- Historical price charts
- Data table with filtering
- CSV export capability
- Stock selection
- System health monitoring

---

## ğŸ§ª Testing

Run the complete test suite:
```bash
source venv/bin/activate

# Test producer â†’ Kafka
python tests/test_producer.py

# Test Kafka messages
python tests/test_kafka.py

# Test PostgreSQL storage
python tests/test_postgres.py

# Test end-to-end pipeline
python tests/test_complete_pipeline.py
```

---

## ğŸ› Troubleshooting

### Producer not sending data
```bash
docker-compose logs producer --tail=20
# Check API key in .env
```

### Spark not processing
```bash
docker-compose logs spark-processor --tail=50
# Verify Kafka topic exists
```

### Dashboard not showing data
```bash
# Check PostgreSQL records
docker exec postgres psql -U stockuser -d stockmarket -c "SELECT COUNT(*) FROM stock_prices;"
```

### Complete system reset
```bash
docker-compose down -v
docker-compose up -d
```

---

## ğŸ¯ Future Enhancements

- [ ] Add technical indicators (RSI, MACD, Bollinger Bands)
- [ ] Implement price alerts (email/Slack)
- [ ] Add ML price prediction model
- [ ] Expand to 50+ stocks
- [ ] Add sentiment analysis from news
- [ ] Create REST API endpoints
- [ ] Deploy to AWS/GCP
- [ ] Add Grafana monitoring

---

## ğŸ“š Blog Series

I wrote a comprehensive 7-part blog series documenting this project:

1. **Part 1:** Introduction & Mac M2 Setup
2. **Part 2:** Building the Kafka Producer
3. **Part 3:** Stream Processing with Spark
4. **Part 4:** PostgreSQL & Redis Storage
5. **Part 5:** Interactive Dashboard with Streamlit
6. **Part 6:** Full Docker Containerization
7. **Part 7:** Free Cloud Deployment

[Read the full series â†’] : [Medium](https://medium.com/@kothagundlarahul)

---

## ğŸ‘¤ Author

**Your Name**
- ğŸ’¼ LinkedIn: [kothagundlarahul](https://linkedin.com/in/kothagundlarahul)
- ğŸ™ GitHub: [@RahulSunny1209](https://github.com/RahulSunny1209)
- ğŸ“§ Email: 2024tracker@gmail.com

---

## ğŸ™ Acknowledgments

- **Finnhub API** for free stock market data
- **Apache Kafka** & **Apache Spark** communities
- **Streamlit** for amazing dashboard framework
- **Docker** for containerization
- Open-source community for incredible tools

---

## ğŸ“„ License

MIT License - feel free to use this project for learning and portfolio purposes!

---

## ğŸ’¼ Hiring?

I'm open to **Data Engineering** and **Software Engineering** opportunities!

This project demonstrates:
- âœ… Real-time data pipeline design
- âœ… Distributed systems (Kafka, Spark)
- âœ… Database optimization
- âœ… Container orchestration
- âœ… Production deployment
- âœ… Full-stack development

**Let's connect!** [LinkedIn](https://linkedin.com/in/kothagundlarahul) | [Email](mailto:2024tracker@gmail.com)

---

**â­ If this project helped you, please star it on GitHub!**

**Built with â¤ï¸ using Python, Kafka, Spark, PostgreSQL, Docker, and Streamlit**
