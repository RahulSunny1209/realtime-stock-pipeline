# ğŸ“ˆ Real-Time Stock Market Data Pipeline

A production-grade, real-time data engineering pipeline that ingests, processes, and visualizes live stock market data.

![Pipeline Architecture](docs/architecture-diagram.png)

## ğŸ¯ Project Overview

This project demonstrates a complete real-time data pipeline using modern data engineering tools and best practices. It fetches live stock data from Polygon.io, streams it through Apache Kafka, processes it with Apache Spark, stores it in PostgreSQL, and visualizes it on a Streamlit dashboard.

## ğŸ—ï¸ Architecture
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Polygon.io API â”‚  â† Live stock data (AAPL, GOOGL, MSFT, AMZN, TSLA)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Kafka Producer  â”‚  â† Fetch every 60s with smart caching
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Apache Kafka    â”‚  â† Message queue (stock-prices topic)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Spark Processor â”‚  â† Calculate moving averages & indicators
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â–¼              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PostgreSQL   â”‚  â”‚  Redis   â”‚  â† Time-series data & cache
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    Streamlit    â”‚  â† Real-time dashboard
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ› ï¸ Tech Stack

- **Language**: Python 3.11.14
- **Message Queue**: Apache Kafka 3.5
- **Stream Processing**: Apache Spark (PySpark 3.5.0)
- **Database**: PostgreSQL
- **Caching**: Redis
- **Visualization**: Streamlit
- **Containerization**: Docker & Docker Compose
- **Data Source**: Polygon.io API
- **Development**: VS Code, Git

## âœ¨ Features

### âœ… Implemented (Phase 1-2)
- [x] Real-time stock data ingestion from Polygon.io
- [x] Apache Kafka streaming pipeline
- [x] Smart metadata caching (6-hour TTL)
- [x] Rate limit protection
- [x] Retry logic with exponential backoff
- [x] Comprehensive logging and metrics
- [x] Kafka UI for monitoring
- [x] Consumer for data verification
- [x] Docker containerization for Kafka stack

### ğŸ”„ In Progress (Phase 3)
- [ ] Apache Spark stream processing
- [ ] Moving averages calculation
- [ ] Technical indicators (RSI, MACD)

### ğŸ“… Planned (Phase 4-8)
- [ ] PostgreSQL time-series storage
- [ ] Redis caching layer
- [ ] Streamlit real-time dashboard
- [ ] AWS cloud deployment
- [ ] CI/CD pipeline
- [ ] Monitoring & alerting

## ğŸš€ Quick Start

### Prerequisites
- MacBook M2 (or compatible Apple Silicon)
- Python 3.11+
- Docker Desktop
- Polygon.io API key (free tier)

### Installation

1. **Clone the repository**
```bash
git clone <your-repo-url>
cd realtime-stock-pipeline
```

2. **Create virtual environment**
```bash
python3.11 -m venv venv
source venv/bin/activate
```

3. **Install dependencies**
```bash
pip install -r requirements.txt
```

4. **Configure environment variables**
```bash
cp .env.example .env
# Edit .env and add your POLYGON_API_KEY
```

5. **Start Kafka cluster**
```bash
docker-compose up -d
```

6. **Run the producer**
```bash
python src/producer/stock_producer.py
```

7. **In another terminal, run the consumer**
```bash
python src/consumer/stock_consumer.py
```

8. **Access Kafka UI**
```
http://localhost:8080
```

## ğŸ“‚ Project Structure
```
realtime-stock-pipeline/
â”œâ”€â”€ README.md
â”œâ”€â”€ docker-compose.yml          # Kafka stack configuration
â”œâ”€â”€ requirements.txt            # Python dependencies
â”œâ”€â”€ .env                        # Environment variables (not committed)
â”œâ”€â”€ .gitignore
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ architecture.md         # Detailed architecture docs
â”‚   â”œâ”€â”€ setup.md               # Setup instructions
â”‚   â””â”€â”€ medium-posts/          # Blog posts
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ producer/
â”‚   â”‚   â””â”€â”€ stock_producer.py  # Kafka producer
â”‚   â”œâ”€â”€ consumer/
â”‚   â”‚   â””â”€â”€ stock_consumer.py  # Kafka consumer
â”‚   â”œâ”€â”€ processing/
â”‚   â”‚   â””â”€â”€ spark_processor.py # Spark stream processing
â”‚   â”œâ”€â”€ storage/
â”‚   â”‚   â””â”€â”€ database.py        # PostgreSQL operations
â”‚   â”œâ”€â”€ dashboard/
â”‚   â”‚   â””â”€â”€ app.py             # Streamlit dashboard
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ config.py          # Configuration utilities
â”‚       â””â”€â”€ logger.py          # Logging setup
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ kafka_config.py        # Kafka configuration
â”‚   â”œâ”€â”€ spark_config.py        # Spark configuration
â”‚   â””â”€â”€ db_config.py           # Database configuration
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ test_pipeline.py       # Unit tests
â””â”€â”€ scripts/
    â”œâ”€â”€ setup_kafka.sh         # Kafka setup script
    â”œâ”€â”€ setup_postgres.sh      # PostgreSQL setup script
    â””â”€â”€ deploy.sh              # Deployment script
```

## ğŸ“Š Current Status

**Phase 2 Complete**: Kafka Producer & Consumer
- âœ… Real-time data flowing through Kafka
- âœ… 5 stocks monitored (AAPL, GOOGL, MSFT, AMZN, TSLA)
- âœ… Production-grade error handling
- âœ… Metrics tracking and monitoring

**Next**: Phase 3 - Spark Stream Processing

## ğŸ“ Key Design Decisions

### Why Polygon.io over Yahoo Finance?
- **Reliability**: Official market data API vs web scraping
- **Rate Limits**: Predictable limits (5 calls/min free tier)
- **Documentation**: Clear API docs and entitlements
- **Production-Ready**: Suitable for real-time systems

### Caching Strategy
- **Metadata TTL**: 6 hours (company info changes rarely)
- **Price Data**: No caching (needs to be real-time)
- **Benefits**: Reduced API calls, respects rate limits

### Retry Logic
- **Exponential Backoff**: 1s, 2s, 4s
- **Max Retries**: 3 attempts
- **Graceful Degradation**: Continues with other symbols

## ğŸ“ Blog Posts

1. [Phase 1: Environment Setup](https://medium.com/@kothagundlarahul/how-i-built-a-real-time-stock-market-data-pipeline-f817e5098e7d)
2. [Phase 2: Kafka Producer] (Coming soon)

## ğŸ‘¨â€ğŸ’» Author

**Rahul Kothagundla**
- MacBook M2
- Learning Data Engineering
- [LinkedIn](your-linkedin) | [Medium](https://medium.com/@kothagundlarahul) | [GitHub](your-github)

## ğŸ“„ License

MIT License

## ğŸ™ Acknowledgments

- Inspired by real-world data engineering practices
- Built for learning and portfolio development
- Special thanks to the data engineering community

---

**â­ If you found this helpful, please star the repository!**

Built with â¤ï¸ on MacBook M2
